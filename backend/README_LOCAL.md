# Genetic Disorder Prediction API (No Docker)

This is a lightweight FastAPI server that loads a trained model bundle exported from your notebook
(scikit-learn Pipeline + metadata) and serves predictions. It can hot-swap models by placing a new
version folder under `models/genetic_disorder/` without changing any API code.

## 1) Create a virtual environment

**Windows (PowerShell):**
```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```

**macOS/Linux (bash/zsh):**
```bash
python3 -m venv .venv
source .venv/bin/activate
```

## 2) Install requirements
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

## 3) Place your model bundle
Export from your notebook as:
```
models/
  genetic_disorder/
    vYYYY-MM-DD_HH-MM/
      pipeline.joblib
      schema.json
      targets.json
      metrics.json
      model_info.yaml
```
Place that under this project's `models/` folder. You can keep multiple versions side-by-side.

## 4) (Optional) Configure environment
Copy `.env.example` to `.env` and adjust if needed:
```bash
# e.g., if your bundles live elsewhere
MODELS_ROOT=/absolute/path/to/bundles

# Local LLM settings (Ollama / other)
LLM_BASE_URL=http://localhost:11434
LLM_MODEL=gemma3
LLM_TIMEOUT=30
```

## 5) Run the API
```bash
uvicorn app.main:app --reload --port 8000
```

## 6) Test endpoints

Health:
```bash
curl http://localhost:8000/health
```

List available models:
```bash
curl http://localhost:8000/models
```

Inspect a model:
```bash
curl http://localhost:8000/models/<model_id>
```

Predict (uses latest by default):
```bash
curl -X POST "http://localhost:8000/predict" ^
     -H "Content-Type: application/json" ^
     -d "{ \"feature1\": 12, \"feature2\": \"Yes\", \"...\": null }"
```
Or pin a specific bundle:
```bash
curl -X POST "http://localhost:8000/predict?model_id=v2025-10-02_01-00"          -H "Content-Type: application/json"          -d '{ "feature1": 12, "feature2": "Yes", "...": null }'
```

The response includes:
- `predictions` per target,
- `confidences` (if available),
- `note` generated by your local Gemma-3 LLM.

## Local Gemma-3
If you use **Ollama** locally:
```bash
ollama pull gemma3
ollama run gemma3
```
The default client expects `http://localhost:11434/api/generate` to be available.

## Hot-swapping models
Add a new version folder under `models/genetic_disorder/` and call `/predict?model_id=<new_id>`
to test it. If `model_id` is omitted, the API picks the lexicographically latest version.
